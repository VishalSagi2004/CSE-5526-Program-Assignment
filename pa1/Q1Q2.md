# Two-Layer Perceptron with Backpropagation and Momentum

This notebook implements a two-layer perceptron with backpropagation to solve the parity problem. The network is trained using different learning rates (`η`) and includes a momentum term (`β = 0.9`) to improve convergence speed.

## Problem Definition
The goal is to classify 4-bit binary inputs based on whether they contain an odd or even number of 1's. If the input has an odd number of 1's, the target output is `1`, and if it has an even number, the target output is `0`.

### Neural Network Architecture
- **Input layer**: 4 binary input nodes.
- **Hidden layer**: 4 hidden units with sigmoid activation.
- **Output layer**: 1 output node with sigmoid activation.

### Stopping Criterion
Training is stopped when the mean absolute error falls below `0.05`.

---

## Varying the Learning Rate (η)

In this section, we vary the value of `η` from `0.05` to `0.5` with an increment of `0.05`. The plot below shows how different learning rates influence the convergence of the model.

### Results:
- For smaller learning rates (`η = 0.05`, `0.1`), the network converges slowly but steadily.
- Learning rates above `0.25` struggle to converge within the error threshold, with larger rates oscillating or taking longer.
- Optimal learning rates for this model appear to be between `0.2` and `0.25`.

---

## Effect of Momentum (β = 0.9)

Momentum (`β = 0.9`) is included in the weight update to accelerate convergence and avoid oscillations. The plot shows how the learning curves behave with momentum applied across various learning rates.

### Results:
- Momentum helps the network avoid local minima and smoothens weight updates, especially for smaller learning rates.
- It is particularly effective when learning rates are in the middle range (`η = 0.2` to `0.25`), significantly speeding up convergence.
